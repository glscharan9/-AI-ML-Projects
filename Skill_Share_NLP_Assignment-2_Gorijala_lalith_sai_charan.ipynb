{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c3ae3a",
   "metadata": {},
   "source": [
    "#  Skill Share NLP Assignment\n",
    "**Student Name:** Gorijala lalith sai charan  \n",
    "**Date:** 2025-06-01\n",
    "\n",
    "---\n",
    "\n",
    "This notebook includes detailed Natural Language Processing (NLP) tasks including tokenization, preprocessing, stopwords removal, POS tagging, Named Entity Recognition, and one-hot encoding using Python libraries such as NLTK and SpaCy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fae9da2",
   "metadata": {},
   "source": [
    "##  Setup and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53b45d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\JC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\JC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\JC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\JC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\JC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#gorijala lalith sai charan \n",
    "# 1-06-2026 \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.tree import Tree\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import spacy\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973703af",
   "metadata": {},
   "source": [
    "##  Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e2e5902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentenceID</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I love programming in Python.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Natural Language Processing is fascinating.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Spacy and NLTK are popular NLP libraries.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Machine learning enables predictive analysis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Data preprocessing is a crucial step in NLP.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SentenceID                                       Sentence\n",
       "0           1                  I love programming in Python.\n",
       "1           2    Natural Language Processing is fascinating.\n",
       "2           3      Spacy and NLTK are popular NLP libraries.\n",
       "3           4  Machine learning enables predictive analysis.\n",
       "4           5   Data preprocessing is a crucial step in NLP."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gorijala lalith sai charan \n",
    "# 1-06-2026\n",
    "# # Load the dataset (make sure the file path is correct)\n",
    "df = pd.read_csv(\"NLP_Assignment_Sentences.csv\")\n",
    "sentences = df['Sentence'].dropna().tolist()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3538e9b7",
   "metadata": {},
   "source": [
    "##  Task 1: Tokenization\n",
    "Tokenization is the process of breaking down text into smaller components such as words or phrases. In this task, we will use `nltk.word_tokenize()` to tokenize each sentence in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d2514cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: ['I', 'love', 'programming', 'in', 'Python', '.']\n",
      "Sentence 2: ['Natural', 'Language', 'Processing', 'is', 'fascinating', '.']\n",
      "Sentence 3: ['Spacy', 'and', 'NLTK', 'are', 'popular', 'NLP', 'libraries', '.']\n",
      "Sentence 4: ['Machine', 'learning', 'enables', 'predictive', 'analysis', '.']\n",
      "Sentence 5: ['Data', 'preprocessing', 'is', 'a', 'crucial', 'step', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "#gorijala lalith sai charan \n",
    "# 1-06-2026\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Assuming sentences is a list of strings\n",
    "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# Display tokenized output\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    print(f\"Sentence {i+1}: {sent}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4464455e",
   "metadata": {},
   "source": [
    "##  Task 1: Tokenization\n",
    "\n",
    "**Objective:**  \n",
    "Break each sentence into individual words (tokens).\n",
    "\n",
    "**Why it matters:**  \n",
    "Tokenization is the first step in NLP. It allows us to process text word by word, enabling downstream tasks like frequency analysis, vectorization, etc.\n",
    "\n",
    "**Method:**  \n",
    "We use `nltk.word_tokenize()` to split each sentence into a list of words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bcc630",
   "metadata": {},
   "source": [
    "## Task 2: Text Preprocessing (Stemming and Lemmatization)\n",
    "In this step, we apply stemming and lemmatization:\n",
    "- **Stemming**: Reduces words to their base or root form (e.g., 'running' â†’ 'run').\n",
    "- **Lemmatization**: Converts words to their base dictionary form (e.g., 'better' â†’ 'good')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b612e0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence 1: I love programming in Python.\n",
      "Stemmed: ['i', 'love', 'program', 'in', 'python', '.']\n",
      "Lemmatized: ['I', 'love', 'programming', 'in', 'Python', '.']\n",
      "\n",
      "Sentence 2: Natural Language Processing is fascinating.\n",
      "Stemmed: ['natur', 'languag', 'process', 'is', 'fascin', '.']\n",
      "Lemmatized: ['Natural', 'Language', 'Processing', 'is', 'fascinating', '.']\n"
     ]
    }
   ],
   "source": [
    "#gorijala lalith sai charan \n",
    "# 1-06-2026\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Use list comprehensions instead of .apply()\n",
    "stemmed_sentences = [[stemmer.stem(word) for word in word_tokenize(sent)] for sent in sentences]\n",
    "lemmatized_sentences = [[lemmatizer.lemmatize(word) for word in word_tokenize(sent)] for sent in sentences]\n",
    "\n",
    "# Compare first 2 examples\n",
    "for i in range(2):\n",
    "    print(f\"\\nSentence {i+1}: {sentences[i]}\")\n",
    "    print(\"Stemmed:\", stemmed_sentences[i])\n",
    "    print(\"Lemmatized:\", lemmatized_sentences[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aecc30c",
   "metadata": {},
   "source": [
    "## Task 2: Text Preprocessing (Stemming and Lemmatization)\n",
    "\n",
    "**Objective:**  \n",
    "Convert words to their base/root form.\n",
    "\n",
    "- **Stemming**: Uses rules to remove prefixes/suffixes (e.g., 'running' â†’ 'run').\n",
    "- **Lemmatization**: Uses a vocabulary and grammar rules to return the base or dictionary form (e.g., 'better' â†’ 'good').\n",
    "\n",
    "**Why it matters:**  \n",
    "Reduces vocabulary size and helps models treat similar words as the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7d24c9",
   "metadata": {},
   "source": [
    "## Task 3: Stopwords Removal\n",
    "Stopwords are common words (like 'is', 'the', 'in') that usually do not add significant meaning. We will remove them using NLTK's stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "74737d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: ['love', 'programming', 'Python', '.']\n",
      "Sentence 2: ['Natural', 'Language', 'Processing', 'fascinating', '.']\n",
      "Sentence 3: ['Spacy', 'NLTK', 'popular', 'NLP', 'libraries', '.']\n",
      "Sentence 4: ['Machine', 'learning', 'enables', 'predictive', 'analysis', '.']\n",
      "Sentence 5: ['Data', 'preprocessing', 'crucial', 'step', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "#gorijala lalith sai charan \n",
    "# 1-06-2026\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# tokenized_sentences is a list of lists (of tokens)\n",
    "cleaned_sentences = [\n",
    "    [word for word in sent if word.lower() not in stop_words]\n",
    "    for sent in tokenized_sentences\n",
    "]\n",
    "\n",
    "# Display cleaned sentences\n",
    "for i, sent in enumerate(cleaned_sentences):\n",
    "    print(f\"Sentence {i+1}:\", sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cb9b7d",
   "metadata": {},
   "source": [
    "## Task 3: Stopwords Removal\n",
    "\n",
    "**Objective:**  \n",
    "Remove common words (like \"is\", \"and\", \"the\") that do not carry meaningful information.\n",
    "\n",
    "**Why it matters:**  \n",
    "Eliminates noise from the dataset and improves model performance by focusing only on the important words.\n",
    "\n",
    "**Tool used:**  \n",
    "`nltk.corpus.stopwords`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4163b586",
   "metadata": {},
   "source": [
    "## Task 4: Part-of-Speech (POS) Tagging\n",
    "Each word will be tagged with its part of speech such as noun (NN), verb (VB), adjective (JJ), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b3300f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence 1:\n",
      "[('I', 'PRP'), ('love', 'VBP'), ('programming', 'VBG'), ('in', 'IN'), ('Python', 'NNP'), ('.', '.')]\n",
      "\n",
      "Sentence 2:\n",
      "[('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('is', 'VBZ'), ('fascinating', 'VBG'), ('.', '.')]\n",
      "\n",
      "Sentence 3:\n",
      "[('Spacy', 'NN'), ('and', 'CC'), ('NLTK', 'NNP'), ('are', 'VBP'), ('popular', 'JJ'), ('NLP', 'NNP'), ('libraries', 'NNS'), ('.', '.')]\n",
      "\n",
      "Sentence 4:\n",
      "[('Machine', 'NN'), ('learning', 'VBG'), ('enables', 'NNS'), ('predictive', 'JJ'), ('analysis', 'NN'), ('.', '.')]\n",
      "\n",
      "Sentence 5:\n",
      "[('Data', 'NNP'), ('preprocessing', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('crucial', 'JJ'), ('step', 'NN'), ('in', 'IN'), ('NLP', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#gorijala lalith sai charan \n",
    "# 1-06-2026\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Assuming tokenized_sentences is a list of token lists\n",
    "pos_tagged = [pos_tag(sent) for sent in tokenized_sentences]\n",
    "\n",
    "# Display POS tagged results\n",
    "for i, tags in enumerate(pos_tagged):\n",
    "    print(f\"\\nSentence {i+1}:\")\n",
    "    print(tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128fd0ad",
   "metadata": {},
   "source": [
    "## Task 4: Part-of-Speech (POS) Tagging\n",
    "\n",
    "**Objective:**  \n",
    "Label each word with its grammatical role, such as noun, verb, adjective, etc.\n",
    "\n",
    "**Why it matters:**  \n",
    "POS tagging helps in syntactic analysis, named entity recognition, and information extraction.\n",
    "\n",
    "**Tool used:**  \n",
    "`nltk.pos_tag()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80477706",
   "metadata": {},
   "source": [
    "##  Task 5: Named Entity Recognition (NER)\n",
    "Named entities are real-world objects such as people, organizations, locations, etc. Weâ€™ll extract them using both **NLTK** and **spaCy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8e3abf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence 1: I love programming in Python.\n",
      " - Python (GPE)\n",
      "\n",
      "Sentence 2: Natural Language Processing is fascinating.\n",
      " - Natural Language Processing (ORG)\n",
      "\n",
      "Sentence 3: Spacy and NLTK are popular NLP libraries.\n",
      " - Spacy (PERSON)\n",
      " - NLTK (ORG)\n",
      " - NLP (ORG)\n",
      "\n",
      "Sentence 4: Machine learning enables predictive analysis.\n",
      "\n",
      "Sentence 5: Data preprocessing is a crucial step in NLP.\n",
      " - NLP (ORG)\n"
     ]
    }
   ],
   "source": [
    "#gorijala lalith sai charan \n",
    "# 1-06-2026\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    doc = nlp(sentence)\n",
    "    print(f\"\\nSentence {i+1}: {sentence}\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\" - {ent.text} ({ent.label_})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e8096f",
   "metadata": {},
   "source": [
    "##  Task 5: Named Entity Recognition (NER)\n",
    "\n",
    "**Objective:**  \n",
    "Identify named entities like people, places, dates, and organizations in text.\n",
    "\n",
    "**Why it matters:**  \n",
    "NER is useful in question answering, knowledge graphs, and summarization.\n",
    "\n",
    "**Tools used:**  \n",
    "- `nltk.ne_chunk()`  \n",
    "- `spacy` model (`en_core_web_sm`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fb1b31",
   "metadata": {},
   "source": [
    "## ðŸ”¢ Task 6: One-Hot Encoding\n",
    "Convert words to binary vectors. Each unique word gets a vector with one '1' and all other '0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "36d9299b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence 1: I love programming in Python.\n",
      "i: [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "love: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "programming: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "in: [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "python: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      ".: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "Sentence 2: Natural Language Processing is fascinating.\n",
      "natural: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "language: [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "processing: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "is: [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "fascinating: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      ".: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "Sentence 3: Spacy and NLTK are popular NLP libraries.\n",
      "spacy: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "and: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "nltk: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "are: [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "popular: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "nlp: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "libraries: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      ".: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#gorijala lalith sai charan \n",
    "# 1-06-2026\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Step 1: Get the first 3 sentences\n",
    "first_3 = sentences[:3]\n",
    "\n",
    "# Step 2: Tokenize and create vocabulary\n",
    "tokens = [word.lower() for s in first_3 for word in word_tokenize(s)]\n",
    "vocab = sorted(set(tokens))\n",
    "\n",
    "# Step 3: Initialize encoder without 'sparse' or 'sparse_output'\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoder.fit([[word] for word in vocab])\n",
    "\n",
    "# Step 4: Transform and display one-hot vectors\n",
    "for i, sentence in enumerate(first_3):\n",
    "    print(f\"\\nSentence {i+1}: {sentence}\")\n",
    "    for word in word_tokenize(sentence.lower()):\n",
    "        if word in vocab:\n",
    "            vector = encoder.transform([[word]]).toarray()  # Convert sparse matrix to dense\n",
    "            print(f\"{word}: {vector}\")\n",
    "        else:\n",
    "            print(f\"{word}: Unknown word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1751daea",
   "metadata": {},
   "source": [
    "##  Task 6: One-Hot Encoding\n",
    "\n",
    "**Objective:**  \n",
    "Convert words into binary vectors where each unique word has its own vector.\n",
    "\n",
    "**Why it matters:**  \n",
    "Machine learning models require numerical input. One-hot encoding is a basic way to represent categorical data like words.\n",
    "\n",
    "**Tool used:**  \n",
    "`sklearn.preprocessing.OneHotEncoder`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
