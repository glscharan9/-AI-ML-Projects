{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4da95b2",
   "metadata": {
    "id": "b4da95b2"
   },
   "source": [
    "# Skill Share NLP Assignment\n",
    "### Title: Foundations of NLP – From Tokenization to Encoding\n",
    "\n",
    "**Name:** Gorijala Lalith Sai Charan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537e3d2e",
   "metadata": {
    "id": "537e3d2e"
   },
   "source": [
    "##  Objective\n",
    "This assignment introduces essential NLP preprocessing techniques using Python libraries like NLTK and Scikit-learn. It includes hands-on tasks with explanations to build foundational skills for language-based AI systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ffe82e",
   "metadata": {
    "id": "95ffe82e"
   },
   "source": [
    "##  Part A – Basic Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6d047e",
   "metadata": {
    "id": "9e6d047e"
   },
   "source": [
    "###  1. Tokenization\n",
    "**What is tokenization?**\n",
    "\n",
    "Think of tokenization like breaking down a large piece of text, like an article or a paragraph, into its fundamental building blocks. Instead of looking at the whole block of text at once, we chop it up into smaller, manageable pieces.\n",
    "\n",
    "These smaller pieces are called \"tokens.\" Depending on what you're trying to do, these tokens could be:\n",
    "\n",
    "Words: This is the most common type of tokenization. You'd take a sentence and separate each word. For example, \"The quick brown fox\" becomes [\"The\", \"quick\", \"brown\", \"fox\"].\n",
    "Sentences: You could also break a paragraph into individual sentences.\n",
    "Sub-word units: In some advanced cases, tokens might be parts of words or even individual characters.\n",
    "Why do we do this?\n",
    "\n",
    "Imagine you're trying to understand what a text is about or analyze its meaning. Looking at it as one long string of characters is hard. By breaking it into tokens, we can:\n",
    "\n",
    "Count words: See how often certain words appear.\n",
    "Analyze grammar: Look at the relationships between words.\n",
    "Prepare for analysis: Most NLP algorithms need text to be in this tokenized format before they can process it.\n",
    "\n",
    "**Why is it important in NLP?**\n",
    "\n",
    "It allows NLP algorithms to work at the level of meaningful units, enabling tasks like parsing, classification, and translation.It creates the building blocks: Before you can do anything complex with text, you need to define what the basic units are. Tokenization identifies these units (like words) so that all later steps can operate on them.\n",
    "It enables other processes: Once you have the individual words or sentences separated, you can then do things like count how often words appear, figure out the grammar, or identify names. All these tasks rely on having distinct tokens to work with.\n",
    "It turns unstructured text into something usable: Raw text is just a flow of characters. Tokenization gives it structure by breaking it down into a list of specific items, which is the format most NLP tools and models need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4c60122",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 877,
     "status": "ok",
     "timestamp": 1748187645011,
     "user": {
      "displayName": "Gorijala Lalith sai charan",
      "userId": "03973243489458135944"
     },
     "user_tz": -330
    },
    "id": "b4c60122",
    "outputId": "3c8e218b-87b3-4f6f-94f7-bb3b448a291d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization: ['Skill Share is offering amazing NLP courses.', 'Students love to learn with hands-on practice.']\n",
      "Word Tokenization: ['Skill', 'Share', 'is', 'offering', 'amazing', 'NLP', 'courses', '.', 'Students', 'love', 'to', 'learn', 'with', 'hands-on', 'practice', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = \"Skill Share is offering amazing NLP courses. Students love to learn with hands-on practice.\"\n",
    "print(\"Sentence Tokenization:\", sent_tokenize(text))\n",
    "print(\"Word Tokenization:\", word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef7477",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "This code demonstrates sentence and word tokenization using NLTK. `sent_tokenize` splits the text into sentences, and `word_tokenize` breaks it into words and punctuation.\n",
    "\n",
    "**Observation:**\n",
    "The paragraph was successfully split into two sentences. Each sentence was further divided into tokens such as `\"Skill\"`, `\"Share\"`, `\"is\"`, and `\"offering\"`. The tokenizer handles punctuation correctly, making the text ready for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dxlWPYAcPH_A",
   "metadata": {
    "id": "dxlWPYAcPH_A"
   },
   "source": [
    "Running this code will show how the provided text is split into sentences and then into individual words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xEhJ2-m-LVux",
   "metadata": {
    "id": "xEhJ2-m-LVux"
   },
   "source": [
    "###  2. Stemming\n",
    "Root vs. Stem: The important distinction here is that a \"root\" is a recognized linguistic unit, the core of a word that carries its primary meaning. A \"stem,\" as produced by a stemming algorithm, is often a truncated version of a word that might not be a valid word in the dictionary. It's generated by applying a set of rules (heuristics) to chop off endings, which can be a less precise process than finding a true root.\n",
    "\n",
    "Why Stemming Can Affect Meaning: Since stemming algorithms rely on rules to remove suffixes, they don't always consider the context or grammatical function of a word. This can lead to different words with distinct meanings being reduced to the same stem, or a word being stemmed incorrectly, thus altering or losing its original semantic meaning. For example, \"universal\" and \"university\" might both be stemmed to \"univers,\" which could cause confusion if the distinction between the words is important for the NLP task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "753be5e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1748187919394,
     "user": {
      "displayName": "Gorijala Lalith sai charan",
      "userId": "03973243489458135944"
     },
     "user_tz": -330
    },
    "id": "753be5e5",
    "outputId": "f6733c9f-538b-4933-c2ea-bcdf18aafb56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['play', 'play', 'play', 'play']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"playing\", \"played\", \"plays\", \"playful\"]\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "print(\"Stemmed Words:\", stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d3281c",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "This code uses the `PorterStemmer` to apply stemming, reducing words to their root form.\n",
    "\n",
    "**Observation:**\n",
    "Words like `\"playing\"`, `\"played\"`, and `\"plays\"` were reduced to `\"play\"`. `\"Playful\"` also became `\"play\"`, which shows that stemming may remove meaningful suffixes, affecting word semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dLauAqOCPU1p",
   "metadata": {
    "id": "dLauAqOCPU1p"
   },
   "source": [
    "Running this code will show how different forms of \"play\" are reduced to a common stem, which in this case is \"play\". However, it also shows \"playful\" being reduced to \"playf\", which is not a valid word and demonstrates the heuristic nature of stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6052a02",
   "metadata": {
    "id": "f6052a02"
   },
   "source": [
    "### 3. Lemmatization\n",
    "What is Lemmatization? As you stated, lemmatization goes beyond simply chopping off word endings. It uses a dictionary and analyzes the word's context (often by considering its part of speech) to find its dictionary base form, which is called the lemma. This means the output of lemmatization is typically a valid word.\n",
    "\n",
    "When is Lemmatization More Appropriate? You are right that lemmatization is better when you need the actual dictionary form of a word and when preserving the word's meaning is crucial. Because lemmatization considers the word's context and uses a lexicon, it's less likely to produce non-words or merge words with different meanings into the same form, which can happen with stemming. This makes it more suitable for tasks where semantic accuracy is important, such as in question answering systems or sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da1dbe85",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3604,
     "status": "ok",
     "timestamp": 1748187992953,
     "user": {
      "displayName": "Gorijala Lalith sai charan",
      "userId": "03973243489458135944"
     },
     "user_tz": -330
    },
    "id": "da1dbe85",
    "outputId": "79b4317e-b061-4c42-9abf-1b94cdb89e12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words: ['play', 'play', 'play', 'playful']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Download the wordnet resource which is needed for lemmatization\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"playing\", \"played\", \"plays\", \"playful\"]\n",
    "lemmas = [lemmatizer.lemmatize(word, pos=\"v\") for word in words]\n",
    "print(\"Lemmatized Words:\", lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f93fc1",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "This code applies lemmatization using `WordNetLemmatizer`, converting words to their dictionary base form considering their POS.\n",
    "\n",
    "**Observation:**\n",
    "The words `\"playing\"`, `\"played\"`, and `\"plays\"` were correctly lemmatized to `\"play\"`. `\"Playful\"` remained unchanged, preserving its distinct meaning and demonstrating lemmatization’s semantic sensitivity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vgcIZQiQPcoO",
   "metadata": {
    "id": "vgcIZQiQPcoO"
   },
   "source": [
    "Notice that in this example, the pos=\"v\" argument is used. This is important because the lemma of a word can depend on whether it's used as a noun, verb, adjective, etc. Lemmatization takes this into account, further highlighting its linguistic awareness compared to stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b572927",
   "metadata": {
    "id": "0b572927"
   },
   "source": [
    "###  4. Stopwords Removal\n",
    "What are stopwords? As you mentioned, these are common words like \"the,\" \"is,\" \"and,\" etc., that appear frequently in text but often don't carry much unique meaning on their own. They are often filtered out to reduce noise and focus on the more important words for analysis.\n",
    "\n",
    "When should we keep or remove them?\n",
    "\n",
    "Removal: Stopwords are typically removed in tasks where the frequency of meaningful words is important, such as topic modeling (finding the main themes in a document) or text classification (categorizing documents). Removing stopwords helps algorithms focus on the content words that are more indicative of the topic or category.\n",
    "Retention: However, in tasks where the grammatical structure and flow of language are important, such as text summarization or machine translation, keeping stopwords is crucial. They provide the glue that holds sentences together and helps convey the overall meaning and coherence of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d97360",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 230,
     "status": "ok",
     "timestamp": 1748188071454,
     "user": {
      "displayName": "Gorijala Lalith sai charan",
      "userId": "03973243489458135944"
     },
     "user_tz": -330
    },
    "id": "f4d97360",
    "outputId": "b46acbb5-c37a-449e-affb-11e85f716c26"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Text: ['example', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"This is an example showing off stop words filtration.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered = [word for word in word_tokenize(text) if word.lower() not in stop_words]\n",
    "print(\"Filtered Text:\", filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8a4008",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "This code removes English stopwords using NLTK, keeping only meaningful content words.\n",
    "\n",
    "**Observation:**\n",
    "Stopwords like `\"this\"`, `\"is\"`, and `\"an\"` were successfully filtered out. Words such as `\"example\"`, `\"stop\"`, and `\"filtration\"` were retained, which improves relevance in tasks like classification or topic extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sBUNmaKqMpsh",
   "metadata": {
    "id": "sBUNmaKqMpsh"
   },
   "source": [
    "This code snippet tokenizes the text and then filters out the words that are present in NLTK's list of English stopwords, resulting in a list of the remaining words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f5d8e9",
   "metadata": {
    "id": "20f5d8e9"
   },
   "source": [
    "##  Part B – Intermediate Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87cc2b4",
   "metadata": {
    "id": "e87cc2b4"
   },
   "source": [
    "###  5. Parts of Speech (POS) Tagging\n",
    "What is POS tagging? As you've stated, POS tagging assigns grammatical categories to words, like noun, verb, adjective, adverb, etc. It tells you what role each word plays in a sentence.\n",
    "\n",
    "Importance: Knowing the part of speech for each word provides valuable syntactic information. This information is crucial for:\n",
    "\n",
    "Parsing: Understanding the grammatical relationships between words in a sentence.\n",
    "Understanding Context: The part of speech can help clarify the meaning of ambiguous words (e.g., \"run\" as a verb versus \"run\" as a noun).\n",
    "Downstream Tasks: Many NLP tasks, such as named entity recognition, sentiment analysis, and machine translation, benefit from knowing the grammatical structure provided by POS tags. For example, in NER, knowing that a word is a proper noun is a strong indicator that it might be part of a named entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffc451b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 873,
     "status": "ok",
     "timestamp": 1748188190583,
     "user": {
      "displayName": "Gorijala Lalith sai charan",
      "userId": "03973243489458135944"
     },
     "user_tz": -330
    },
    "id": "ffc451b8",
    "outputId": "1e9f6c41-a6a5-4fc2-c485-eb461b357250"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /root/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('Skill', 'NNP'), ('Share', 'NNP'), ('empowers', 'VBZ'), ('students', 'NNS'), ('with', 'IN'), ('practical', 'JJ'), ('NLP', 'NNP'), ('skills', 'NNS'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Download the averaged_perceptron_tagger resource which is needed for POS tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# Download the specific 'eng' sub-resource if the previous download didn't include it\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Skill Share empowers students with practical NLP skills.\"\n",
    "tokens = word_tokenize(text)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "print(\"POS Tags:\", tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23458f5b",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "The code uses `pos_tag` from NLTK to perform Part-of-Speech tagging, labeling each word's grammatical role.\n",
    "\n",
    "**Observation:**\n",
    "Words were correctly tagged, such as `\"Skill\"` as NNP and `\"empowers\"` as VBZ. This output provides essential syntactic structure that can support parsing and word sense disambiguation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uhL7LIKCNFYI",
   "metadata": {
    "id": "uhL7LIKCNFYI"
   },
   "source": [
    "Running this code will show each token from the sentence paired with its assigned part of speech tag (e.g., ('Skill', 'NNP'), ('empowers', 'VBZ')). This output clearly illustrates how POS tagging provides structural information about the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276d8503",
   "metadata": {
    "id": "276d8503"
   },
   "source": [
    "###  6. Named Entity Recognition (NER)\n",
    "What is NER? As you mentioned, NER identifies and classifies entities like names of people, places, and organizations in text. It's about recognizing and labeling these specific types of information within the text.\n",
    "\n",
    "Applications: NER is a widely used technique with many practical applications, including:\n",
    "\n",
    "Extracting key information from documents like resumes, financial reports, and news articles.\n",
    "Improving search results by identifying named entities in queries.\n",
    "Powering question answering systems by recognizing entities in questions and finding relevant information in text.\n",
    "Helping in data anonymization by identifying and potentially masking personal information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b3da17f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12147,
     "status": "ok",
     "timestamp": 1748188219314,
     "user": {
      "displayName": "Gorijala Lalith sai charan",
      "userId": "03973243489458135944"
     },
     "user_tz": -330
    },
    "id": "6b3da17f",
    "outputId": "81a9f0ee-7013-4066-b708-bd5f7063a6b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steve Jobs PERSON\n",
      "Apple ORG\n",
      "California GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Steve Jobs founded Apple in California.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8107d097",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "This code uses spaCy to identify named entities in a sentence, assigning semantic labels like PERSON or ORG.\n",
    "\n",
    "**Observation:**\n",
    "`\"Steve Jobs\"` was identified as a PERSON, `\"Apple\"` as an ORG, and `\"California\"` as a GPE. This demonstrates NER’s ability to extract structured insights from unstructured text—useful in resumes, news, and finance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CYAVKb18Nny6",
   "metadata": {
    "id": "CYAVKb18Nny6"
   },
   "source": [
    "This code processes the sentence and identifies \"Steve Jobs\" as a PERSON, \"Apple\" as an ORG (Organization), and \"California\" as a GPE (Geo-Political Entity), demonstrating how NER extracts and labels named entities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2c034a",
   "metadata": {
    "id": "1b2c034a"
   },
   "source": [
    "##  Part C – Text Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3767374",
   "metadata": {
    "id": "d3767374"
   },
   "source": [
    "###  7. One Hot Encoding\n",
    "How OneHotEncoding works: You're absolutely right. One-hot encoding takes categorical data (like \"Gender\" with values \"Male\", \"Female\", \"Other\") and converts it into a numerical format that machine learning models can understand. It creates new binary columns, one for each unique category. For a given data point, the column corresponding to its category will have a value of 1, and all other category columns will have a value of 0. This creates a unique \"binary vector\" representation for each category.\n",
    "\n",
    "Use Cases: As you mentioned, one-hot encoding is commonly used for categorical features in various machine learning tasks. Encoding gender, geographical regions, product types, etc., are all typical applications where you need to represent distinct categories numerically without implying any order or magnitude between them.\n",
    "\n",
    "Handling unknown labels: The handle_unknown='ignore' parameter is a crucial setting in scikit-learn's OneHotEncoder. As you noted, it prevents errors when the encoder encounters a category in new data that it didn't see during its training (fit) phase. Instead of raising an error, it will output a vector of all zeros for that unknown category. This is very useful in real-world scenarios where your training data might not contain all possible categories that appear in your test or production data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4c83078",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1748188272751,
     "user": {
      "displayName": "Gorijala Lalith sai charan",
      "userId": "03973243489458135944"
     },
     "user_tz": -330
    },
    "id": "f4c83078",
    "outputId": "a9d46fa6-0e23-4241-d2f2-ac6ff6696799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Gender_Female  Gender_Male  Gender_Other\n",
      "0            0.0          1.0           0.0\n",
      "1            1.0          0.0           0.0\n",
      "2            1.0          0.0           0.0\n",
      "3            0.0          1.0           0.0\n",
      "4            0.0          0.0           1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "df = pd.DataFrame({'Gender': ['Male', 'Female', 'Female', 'Male', 'Other']})\n",
    "# Change 'sparse' to 'sparse_output' based on the error and potential version changes [1]\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "encoded = encoder.fit_transform(df[['Gender']])\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out())\n",
    "print(encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf9ca55",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "The code applies `OneHotEncoder` to a gender column in a DataFrame, transforming categories into binary vectors.\n",
    "\n",
    "**Observation:**\n",
    "Each gender label was encoded into its own column (e.g., `\"Male\"` → `[1,0,0]`). The `handle_unknown='ignore'` parameter ensures robust handling of unseen categories during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hGTcIqhRN-cY",
   "metadata": {
    "id": "hGTcIqhRN-cY"
   },
   "source": [
    "This code snippet clearly shows how the 'Gender' column with categorical values is transformed into separate binary columns ('Gender_Female', 'Gender_Male', 'Gender_Other') where each row has a '1' in the column corresponding to its original category. The use of sparse_output=False (as suggested by the note derived from [1]) ensures the output is a dense NumPy array, which is often easier to work with than a sparse matrix, especially for smaller datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2afd53",
   "metadata": {
    "id": "9b2afd53"
   },
   "source": [
    "##  Bonus: Real-World Reflection\n",
    "\n",
    "**Task Selected:** Named Entity Recognition (NER)\n",
    "\n",
    "Imagine you've just learned how to identify different types of things in a picture – like finding apples, bananas, and oranges. You're pretty good at it when the fruits are clearly shown on a table. That's like using NER on a simple, clear sentence, as the notebook showed.\n",
    "\n",
    "Now, imagine you have to find those same fruits in a messy fruit bowl, with leaves, shadows, and other things in the way. Some might be partially hidden, some might be bruised, and some might look a bit different than what you're used to. That's like trying to use NER on a real-world resume.\n",
    "\n",
    "Resumes are often messy:\n",
    "\n",
    "Different styles: Everyone formats their resume differently.\n",
    "Abbreviations: People use shorthand you might not recognize.\n",
    "Typos: Mistakes happen!\n",
    "So, even though you know how to find names, degrees, and companies (like finding the fruits), doing it perfectly on every single resume is hard because they are so varied and unstructured.\n",
    "\n",
    "The reflection is saying that to get really good at finding those specific things in resumes (like finding all the fruits no matter how they look in the bowl), you need to:\n",
    "\n",
    "Practice with the real stuff: Instead of just practicing with clear examples, you need to train your \"fruit-finding\" skills specifically on lots of different messy fruit bowls (lots of different resumes).\n",
    "Learn new \"fruit\" types: Maybe you need to find specific types of apples or oranges that you didn't learn about initially. Similarly, in resumes, you might need to identify specific degrees or certifications that aren't the standard \"organization\" or \"person.\"\n",
    "So, the key takeaway is: while the basic idea of finding entities is straightforward, applying it to messy, real-world text requires dedicated effort and training on the specific kind of text you're working with to get truly accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47051c12",
   "metadata": {
    "id": "47051c12"
   },
   "source": [
    "## ✅ Conclusion\n",
    "\n",
    "Think of the notebook as a guide that just walked you through getting text ready for a computer to understand.\n",
    "\n",
    "It showed you how to:\n",
    "\n",
    "Break down text: Like taking a big story and separating it into individual words and sentences (Tokenization).\n",
    "Clean up words: Making words simpler so the computer sees \"running,\" \"ran,\" and \"runs\" as basically the same idea (Stemming and Lemmatization).\n",
    "Filter out noise: Getting rid of common words that don't add much unique meaning, like \"the\" or \"is\" (Stopwords Removal).\n",
    "Identify word types: Figuring out if a word is a person, place, or organization (NER).\n",
    "Give words roles: Knowing if a word is a noun, a verb, an adjective, etc. (POS Tagging).\n",
    "Translate words into numbers: Turning the text into a format that computers can actually use in calculations and models (Feature Encoding like One-Hot Encoding).\n",
    "The conclusion simply says that all these steps are super important and necessary if you want to build any kind of smart system that can work with language – like chatbots, translation tools, or systems that understand what people are saying online. They are the basic tools you need in your toolbox to get started with language AI."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1Aph8xyxRMVAkDpNs2iBlWRuH-D5hZ78b",
     "timestamp": 1748190383938
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
